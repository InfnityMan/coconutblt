# Suggested model configuration for ~1B parameters (approximate)
model:
  vocab_size: 256
  d_model: 2048
  n_layers: 20
  n_heads: 16
  mlp_ratio: 4
  patch_size: 8
  patcher: entropy
  entropy_patcher:
    min_patch_size: 2
    max_patch_size: 16
    lookahead: 4
    entropy_threshold: 3.0
    # set estimator: 'learned' to use LearnedEntropyEstimator in prototype
    estimator: learned
  use_byte_patch_attention: true
  n_latents: 16
  latent_dim: 2048
  latent_interval: 4

training:
  batch_size_training: 8
  lr: 1e-4
  bf16: true
  optimizer: adamw
  # estimator training options (optional)
  train_estimator: true
  est_loss_weight: 1.0
  est_lr: 1e-4
